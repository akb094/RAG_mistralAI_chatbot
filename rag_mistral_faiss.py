# -*- coding: utf-8 -*-
"""RAG_mistral_FAISS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12QdC3wwxkBupt3rVBzIPZgVxkfi2JHhi

Installing Dependencies
"""

!pip install -q langchain mistralai==0.4.2 faiss-cpu tiktoken sentence-transformers

"""Set Mistral API Key"""

import os
from google.colab import userdata

# Load API key from Colab secrets
os.environ["MISTRAL_API_KEY"] = userdata.get("MISTRAL_API_KEY")

# Check success
if os.getenv("MISTRAL_API_KEY"):
    print("âœ… Mistral API key loaded successfully.")
else:
    raise ValueError("âŒ MISTRAL_API_KEY not found. Please set it in Colab secrets.")

"""Load and Prepare COVID-19 FAQ Dataset"""

import requests
import json

url = "https://raw.githubusercontent.com/deepset-ai/COVID-QA/master/data/question-answering/COVID-QA.json"
response = requests.get(url)
data = response.json()

# Parse Q&A pairs
qa_pairs = []
for article in data["data"]:
    for paragraph in article["paragraphs"]:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            answer = qa["answers"][0]["text"] if qa["answers"] else "No answer provided"
            qa_pairs.append({"question": question, "answer": answer, "context": context})

# Preview
for item in qa_pairs[:3]:
    print(f"Q: {item['question']}\nA: {item['answer']}\n")

"""Convert to LangChain Documents"""

from langchain_core.documents import Document

documents = [
    Document(page_content=f"Context: {item['context']}\nQ: {item['question']}\nA: {item['answer']}")
    for item in qa_pairs
]

"""Text Splitting and Vector Store"""

!pip install langchain langchain_community
!pip install -q tqdm

from tqdm.auto import tqdm
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# Limit documents to first 300 for speed (remove this limit later)
limited_documents = documents[:300]

print(f"Original documents: {len(documents)}")
print(f"Using limited documents: {len(limited_documents)}")

# Bigger chunks to reduce number of splits
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

# Split with progress bar
docs = []
for doc in tqdm(limited_documents, desc="Splitting documents"):
    docs.extend(splitter.split_documents([doc]))

print(f"Total chunks after splitting: {len(docs)}")

# Embedding model
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# To speed up embedding, embed documents in batches with progress bar
batch_size = 50
all_embeddings = []
for i in tqdm(range(0, len(docs), batch_size), desc="Embedding chunks"):
    batch = docs[i : i + batch_size]
    # embed batch
    embeddings = embedding_model.embed_documents([d.page_content for d in batch])
    all_embeddings.extend(embeddings)

# Build FAISS index with precomputed embeddings
vectorstore = FAISS.from_documents(docs, embedding_model)
retriever = vectorstore.as_retriever()

print("âœ… FAISS index built successfully!")

import mistralai
print(mistralai.__version__)

"""Use Mistral LLM with LangChain"""

from mistralai.client import MistralClient
from langchain.llms.base import LLM # Import the base LLM class
from pydantic import Field, ConfigDict # Import Field and ConfigDict for Pydantic models
from typing import Optional, List, Mapping, Any # Import necessary typing hints
import os # Import os

class MistralAI(LLM):
    model: str = Field(default="mistral-tiny")
    temperature: float = Field(default=0.7)
    api_key: Optional[str] = Field(default=None)

    # Configure Pydantic to allow extra attributes (like 'client')
    model_config = ConfigDict(extra='allow')

    def __init__(self, **data):
        super().__init__(**data)
        self.api_key = self.api_key or os.getenv("MISTRAL_API_KEY")
        # Initialize the MistralClient from the correctly installed version (0.4.2)
        self.client = MistralClient(api_key=self.api_key)

    @property
    def _llm_type(self) -> str:
        return "mistral"

    @property # Add the @property decorator here
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model": self.model, "temperature": self.temperature}

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.client.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
        )
        # Access the message content from the response object
        return response.choices[0].message.content # Use .content instead of dictionary access

"""Simple Chatbot Interface"""

from langchain.chains import RetrievalQA # Import RetrievalQA

# Initialize the custom MistralAI LLM
llm = MistralAI(model="mistral-tiny", temperature=0)

# Create the RetrievalQA chain
qa_chain = RetrievalQA.from_llm(llm=llm, retriever=retriever, return_source_documents=True)

def rag_chat():
    print("ðŸ’¬ RAG Chatbot (COVID-19 FAQ + Mistral)")
    print("Type 'exit' to quit.\n")
    while True:
        query = input("You: ")
        if query.lower() == "exit":
            break
        result = qa_chain({"query": query})
        print("Bot:", result['result'])
        print("-" * 50)

# Run the chatbot
rag_chat()

"""Save Sample Questions to Excel"""

import pandas as pd

sample_questions = [
    "What are the symptoms of COVID-19?",
    "How does the virus spread?",
    "Can asymptomatic people spread COVID-19?",
    "Is there a vaccine available?",
    "What are preventive measures?"
]

# Use the qa_chain as a callable and access the 'result' key
qa_pairs = [{"Question": q, "Answer": qa_chain({"query": q})['result']} for q in sample_questions]

df = pd.DataFrame(qa_pairs)
df.to_excel("rag_chatbot_responses.xlsx", index=False)
print("âœ… Saved to rag_chatbot_responses.xlsx")

"""Streamlit Code"""

# app.py
!pip install streamlit # Install Streamlit
import streamlit as st

st.title("COVID-19 RAG Chatbot")

query = st.text_input("Ask a question:")

if query:
    # qa_chain expects a dictionary input, so pass the query in the correct format
    result = qa_chain({"query": query})
    st.write("Answer:", result['result'])